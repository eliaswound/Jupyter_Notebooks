{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bba409-00be-4730-8fbe-52bdcd96a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is version 3.10 for smFISH spot detection. I have tried using pycharm + other python platform but in the end I feel like using Jupyter notebook is more suitable \n",
    "# Especailly for person who enter the Planarian Field might not understand coding/structure and other things too much. \n",
    "# This requires a enviornment and setting parameters. \n",
    "# https://stackoverflow.com/questions/58645807/change-interpreter-in-jupyter-notebook Please refer to this for setting up python interpreter. \n",
    "# This requires setting up a enviornment for 3D detection on stardist and bigfish. \n",
    "# Recommend to setup a enviornment for Stardist and then install bigfish/fishquant. \n",
    "# Please read instructions from Stardist to do so, and set python interpreter using the stackoverflow instructions. \n",
    "# Depend on your Image and settings this whole code need a while to run. I am making it as light as possible so please be patient. \n",
    "# I am also using tqdm in all of my customized code to show you process and the expected time in real time. \n",
    "# Please contact qingxuguan2020@u.northwestern.edu for any details/updates/help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b760c9b2-2894-49a4-8d2f-31057411b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleiChannelPath = \"/Users/eliasguan/Desktop/EG_0920_Test_wnt1_incision_amputation/Experiment_dataset/Experiment/0h_Amputation/Image1/405/0h_Amputation_Image1_405.tif\"\n",
    "modelDirectory = \"/Users/eliasguan/Desktop/models\"\n",
    "resultsPath = \"/Users/eliasguan/Desktop/EG_0920_Test_wnt1_incision_amputation/Experiment_dataset/Experiment/0h_Amputation/Image1/565/results\"\n",
    "nuclei_projection_size = 10\n",
    "# Set your thresh hold for stardist segmentation \n",
    "detection_thresh = (0.4,0.3)\n",
    "# If the image is normalized (i.e normalized from CSBdeep or other method, use True, or use False)\n",
    "normalizedImage = False\n",
    "# Change your normalize threshold here. If it is True previously, you dont need to change it. \n",
    "normalize_thresh = (0.1, 99.9)\n",
    "# Run all then you are good.\n",
    "# Do you want to use a custo trained model here. \n",
    "# Here if you need to have a custom model the system will ask you for a custom model directory\n",
    "custom_model = True\n",
    "# Here select your random sample percentage in z-stack for nuclei segmentation estimation. 0 -100 is the percentage range, 100% means each z-stack will be selected and sampled. \n",
    "samplePercentage = 100\n",
    "# Here select your random sample region size for nuclei segmentation estimation. 500X5000 means 500 X 500 pixel in each z-stack will be selected\n",
    "sampleRegionSize = (500,500)\n",
    "# Here set the nuclei label expanding radius\n",
    "labelExpansionSize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59013d7-ebd8-4d0e-96ab-31315fa4794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is for importing all the functions for smFISH detection. Please install them if you dont have these pacakges. \n",
    "import os\n",
    "import sys\n",
    "# import tk for getting the directory faster. dont need this in a command line/server version\n",
    "import numpy as np\n",
    "import glob\n",
    "from tifffile import imread, imwrite\n",
    "import bigfish.detection \n",
    "import bigfish.stack\n",
    "import bigfish.plot\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "# if you dont need to plot in jupyter you don need these. Some magic interperters need to be removed for command line version. \n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"image.interpolation\"] = 'none'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# csb deep is to take normalization \n",
    "from csbdeep.utils import Path, normalize\n",
    "from csbdeep.io import save_tiff_imagej_compatible\n",
    "# This is your stardist models and everything in stardist coming from. \n",
    "from stardist import random_label_cmap, _draw_polygons, export_imagej_rois\n",
    "from stardist.models import StarDist2D\n",
    "from skimage import segmentation\n",
    "import bigfish.multistack as multistack\n",
    "# Set random seed for you color map. You do not really need this to be 6 all the time, but its okay. \n",
    "np.random.seed(6)\n",
    "lbl_cmap = random_label_cmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa1ddb7-f0d3-4609-9d7a-f555745bdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_file(filename):\n",
    "    try:\n",
    "        data = np.load(filename)\n",
    "        print(f\"Loaded {filename} successfully.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{filename} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "    return None\n",
    "def make_stardist_Predictions_labels (dataset, normalized = False, normalize_low = 0, normalize_high = 0, nms_thresh = 0, prob_thresh = 0): \n",
    "    ''' input: csbdeep input: If this image is a csbdeep filtered image. If this is true, the image will not be normalized. \n",
    "               dataset: The data set you want to analysis on \n",
    "               normalize_low and normalized_high: The parameter you want to normalize, if either of them is 0 will use default value (1,99.8)\n",
    "               if not then will use these values \n",
    "               nms_thresh and prob_thrsh : the parameter of overlapping and probablity threshold. If eitehr of them is 0 will use default value \n",
    "               for the model (depend on the model) if not then use these values\n",
    "    '''\n",
    "    labels_collection= []\n",
    "    for i in range(len(dataset)): \n",
    "        if not normalized:\n",
    "            if normalize_low ==0 or normalize_high == 0:\n",
    "                img = normalize(dataset[i], 1,99.8, axis=(0,1))\n",
    "            else:\n",
    "                img = normalize(dataset[i], normalize_low, normalize_high, axis=(0,1))\n",
    "        else:\n",
    "            img = dataset[i]\n",
    "        if nms_thresh == 0 or prob_thresh == 0 :\n",
    "            labels, details = model.predict_instances(img)\n",
    "        else: \n",
    "            labels, details = model.predict_instances(img,nms_thresh = nms_thresh, prob_thresh = prob_thresh)\n",
    "        # write labels\n",
    "        labels_collection.append(labels)\n",
    "        export_imagej_rois('polygons/polygon_rois_'+str(i).zfill(3)+'.zip', details['coord'])\n",
    "        imwrite(\"labels/Nucleus_Labels_\"+str(i).zfill(3)+\".tif\", labels)\n",
    "    labels_collection = np.array(labels_collection, dtype = np.uint16)\n",
    "    return labels_collection\n",
    "def random_select_images(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Randomly select a certain percentage of images from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (list): A list containing 2D arrays (images).\n",
    "        percentage (float): Percentage of images to be selected.\n",
    "\n",
    "    Returns:\n",
    "        selected_indices (list): A list of indices corresponding to the selected images.\n",
    "    \"\"\"\n",
    "    num_images = len(dataset)\n",
    "    num_selected = int(np.ceil(percentage/100 * num_images))\n",
    "    selected_indices = np.random.choice(num_images, num_selected, replace=False)\n",
    "    return selected_indices\n",
    "\n",
    "def random_select_region(image, region_size):\n",
    "    \"\"\"\n",
    "    Randomly select a region within the image with a certain size.\n",
    "\n",
    "    Parameters:\n",
    "        image (2D array): The original image.\n",
    "        region_size (tuple): The size of the region to be selected (height, width).\n",
    "\n",
    "    Returns:\n",
    "        region (2D array): The selected region.\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape\n",
    "    region_height, region_width = region_size\n",
    "\n",
    "    if region_height > image_height or region_width > image_width:\n",
    "        raise ValueError(\"Region size exceeds original image size.\")\n",
    "\n",
    "    start_row = np.random.randint(0, image_height - region_height + 1)\n",
    "    start_col = np.random.randint(0, image_width - region_width + 1)\n",
    "    end_row = start_row + region_height\n",
    "    end_col = start_col + region_width\n",
    "\n",
    "    region_prop = [(start_row, end_row),(start_col,end_col)]\n",
    "    return region_prop\n",
    "def make_random_examples(dataset, normalize_low = 0, normalize_high =0, nms_thresh =0, prob_thresh = 0, normalized = False, percentage = 20, region_size = (500,500)):\n",
    "    selected_indicies = sorted(random_select_images(dataset, percentage))\n",
    "    for item in selected_indicies: \n",
    "        if normalized:\n",
    "            img = dataset[item]\n",
    "        else:\n",
    "            if normalize_low ==0 or normalize_high == 0:\n",
    "                img = normalize(dataset[item], 1,99.8, axis=(0,1))\n",
    "            else:\n",
    "                img = normalize(dataset[item], normalize_low, normalize_high, axis=(0,1))   \n",
    "        if nms_thresh == 0 or prob_thresh == 0 :\n",
    "            labels, details = model.predict_instances(img)\n",
    "        else: \n",
    "            labels, details = model.predict_instances(img, nms_thresh = nms_thresh, prob_thresh = prob_thresh)\n",
    "        region_props =  random_select_region(img, region_size)\n",
    "        cropped_image = img[region_props[0][0]:region_props[0][1],region_props[1][0]:region_props[1][1]]\n",
    "        cropped_label = labels[region_props[0][0]:region_props[0][1],region_props[1][0]:region_props[1][1]]\n",
    "        figure = plt.figure(figsize=(13,10))\n",
    "        coord, points, prob = details['coord'], details['points'], details['prob']\n",
    "        # Plot image on the first one\n",
    "        ax1 = figure.add_subplot(121); ax1.imshow(cropped_image, cmap='gray'); ax1.axis('off')\n",
    "        # Plot image on the second one\n",
    "        ax2 = figure.add_subplot(122); ax2.imshow(cropped_image, cmap='gray'); ax2.axis('off')\n",
    "        # Plot labels on the third one. \n",
    "        ax2.imshow(cropped_label, cmap=lbl_cmap, alpha=0.3)\n",
    "        figure.tight_layout()\n",
    "        plt.savefig(\"random_examples/random_example_\"+str(item).zfill(3)+\".tif\")\n",
    "def create_folder_in_same_directory(file_path, folder_name):\n",
    "    \"\"\"\n",
    "    Creates a folder with the specified name in the same directory as the given file.\n",
    "    If the folder already exists, it returns the existing path.\n",
    "    \"\"\"\n",
    "    # Get the directory of the given file\n",
    "    directory = os.path.dirname(file_path)\n",
    "    \n",
    "    # Define the path for the specified folder\n",
    "    folder_path = os.path.join(directory, folder_name)\n",
    "    \n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        # Create the folder if it doesn't exist\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Created '{folder_name}' folder at: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"'{folder_name}' folder already exists at: {folder_path}\")\n",
    "    \n",
    "    return folder_path\n",
    "def generate_max_projection_array(array, projection_size):\n",
    "    ranges = []\n",
    "    total = array.shape[0]\n",
    "    projected_image = []\n",
    "    for i in range(0, total, projection_size):\n",
    "        start = i\n",
    "        end = min(i + projection_size - 1, total - 1)\n",
    "        ranges.append((start, end))\n",
    "    for item in ranges:\n",
    "        start, end = item\n",
    "        nuclei_array = []\n",
    "        for i in range(start,end):\n",
    "            nuclei_array.append(array[i])\n",
    "        projection = bigfish.stack.maximum_projection(np.array(nuclei_array,dtype=np.uint8))\n",
    "        projected_image.append(projection)\n",
    "    return np.array(projected_image,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83f1c91-6ae0-4e04-b46b-22e867af61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spots_post_decomposition_and_background_removed.npy successfully.\n"
     ]
    }
   ],
   "source": [
    "# First read in spots: \n",
    "os.chdir(resultsPath)\n",
    "# File names\n",
    "file_A = 'spots_post_decomposition_and_background_removed.npy'\n",
    "file_B = 'spots_post_decomposition.npy'\n",
    "\n",
    "# Try loading A, fallback to B if A fails\n",
    "post_decomposition_array = load_npy_file(file_A)\n",
    "if post_decomposition_array is None:\n",
    "    post_decomposition_array = load_npy_file(file_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c540e1-633e-48dc-8c2e-9aee327893a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 22:11:56.791434: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-10-18 22:11:56.791457: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-10-18 22:11:56.791462: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-10-18 22:11:56.792301: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-18 22:11:56.793063: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.492714, nms_thresh=0.3.\n",
      "'labels' folder already exists at: labels\n",
      "'polygons' folder already exists at: polygons\n",
      "'random_examples' folder already exists at: random_examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'random_examples'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nucleiImageArray = imread(nucleiChannelPath)\n",
    "nucleiImageArray_projected = generate_max_projection_array(nucleiImageArray, projection_size = nuclei_projection_size)\n",
    "imwrite('nuclei_projection.tif',nucleiImageArray_projected, photometric = 'minisblack')\n",
    "if custom_model:\n",
    "    model = StarDist2D(None, name='stardist_2D', basedir=modelDirectory)\n",
    "else: \n",
    "    model = StarDist2D.from_pretrained('2D_versatile_fluo')\n",
    "create_folder_in_same_directory('.','labels')\n",
    "create_folder_in_same_directory('.','polygons')\n",
    "create_folder_in_same_directory('.','random_examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993fdfd-3bc7-412f-82d7-5dd729d9ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 22:11:57.736295: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "nucleiImageArray_projected_labels = make_stardist_Predictions_labels(nucleiImageArray_projected, normalized = normalizedImage, normalize_low = normalize_thresh[0], normalize_high = normalize_thresh[1], nms_thresh = detection_thresh[0], prob_thresh = detection_thresh[1])\n",
    "make_random_examples(nucleiImageArray_projected, normalized = normalizedImage, normalize_low = normalize_thresh[0], normalize_high = normalize_thresh[1], nms_thresh = detection_thresh[0], prob_thresh = detection_thresh[1], percentage = samplePercentage, region_size = sampleRegionSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec4400-5c7c-4c3d-9685-b182d88e8463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b929052-2f09-49d7-802c-51944543a113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stardist-env",
   "language": "python",
   "name": "stardist-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
